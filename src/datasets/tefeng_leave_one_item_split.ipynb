{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ta-Feng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users/tr.zaiqm/mycode/tr_rec/configs/\n",
      "/users/tr.zaiqm/mycode/tr_rec/datasets/\n",
      "/users/tr.zaiqm/mycode/tr_rec/checkpoints/\n",
      "/users/tr.zaiqm/mycode/tr_rec/results/\n",
      "/users/tr.zaiqm/mycode/tr_rec/samples/\n",
      "/users/tr.zaiqm/mycode/tr_rec/logs/\n",
      "/users/tr.zaiqm/mycode/tr_rec/runs/\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append(\"../../\")\n",
    "import random\n",
    "from pandas.core.frame import DataFrame\n",
    "import sklearn\n",
    "from utils.unigramTable import UnigramTable\n",
    "\n",
    "\n",
    "DEFAULT_USER_COL = \"user_ids\"\n",
    "DEFAULT_ITEM_COL = \"item_ids\"\n",
    "DEFAULT_ORDER_COL = \"order_ids\"\n",
    "DEFAULT_RATING_COL = \"ratings\"\n",
    "DEFAULT_LABEL_COL = \"label\"\n",
    "DEFAULT_TIMESTAMP_COL = \"timestamp\"\n",
    "DEFAULT_PREDICTION_COL = \"prediction\"\n",
    "DEFAULT_FLAG_COL = \"flag\"\n",
    "data_base_dir = \"../../datasets/tafeng/\"\n",
    "\n",
    "\n",
    "test_percents = [0.1, 0.15, 0.2, 0.25, 0.3]\n",
    "# validate percent just choose the same percent in the training set\n",
    "\n",
    "negative_size = 100\n",
    "\n",
    "min_u_c = 10  # items which were purcharsed by at least min_u_c users\n",
    "min_i_c = 10  # users buy at least min_i_c items\n",
    "min_o_c = 10  ##users have at least min_o_c orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load full data to sequence DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405742\n",
      "464118\n"
     ]
    }
   ],
   "source": [
    "original_train_file = data_base_dir + \"train.txt\"\n",
    "original_test_file = data_base_dir + \"test.txt\"\n",
    "prior_test_df = DataFrame(columns=[\"order_id\", \"user_id\", \"time\", \"item_id\"])\n",
    "prior_train_df = DataFrame(\n",
    "    columns=[\"order_id\", \"user_id\", \"time\", \"item_id\"]\n",
    ")  # initial dataframe\n",
    "interaction_list = []\n",
    "with open(original_train_file) as ori_test_df:\n",
    "    for line in ori_test_df:\n",
    "        temp_list = line.replace(\"\\n\", \"\\t\").split(\n",
    "            \"\\t\"\n",
    "        )  # replace '\\n' in the end of the line by '\\t'\n",
    "        # split line by '\\t'\n",
    "        # store splited items in a list\n",
    "        order_id = temp_list[0]\n",
    "        item_ids_list = temp_list[1:-3]  # itemids\n",
    "        time_order = temp_list[-2].replace(\"-\", \"\")\n",
    "        user_id = temp_list[-3]\n",
    "        for item_id in item_ids_list:\n",
    "            interaction_list.append(\n",
    "                [order_id, user_id, item_id, \"train\", \"1\", time_order]\n",
    "            )\n",
    "    print(len(interaction_list))\n",
    "with open(original_test_file) as ori_test_df:\n",
    "    for line in ori_test_df:\n",
    "        temp_list = line.replace(\"\\n\", \"\\t\").split(\n",
    "            \"\\t\"\n",
    "        )  # replace '\\n' in the end of the line by '\\t'\n",
    "        # split line by '\\t'\n",
    "        # store splited items in a list\n",
    "        order_id = temp_list[0]\n",
    "        item_ids_list = temp_list[1:-3]  # itemids\n",
    "        time_order = temp_list[-2].replace(\"-\", \"\")\n",
    "        user_id = temp_list[-3]\n",
    "        for item_id in item_ids_list:\n",
    "            interaction_list.append(\n",
    "                [order_id, user_id, item_id, \"train\", \"1\", time_order]\n",
    "            )\n",
    "    print(len(interaction_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(464118, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions = np.array(interaction_list)\n",
    "interactions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.DataFrame(\n",
    "    data={\n",
    "        DEFAULT_ORDER_COL: interactions[:, 0],\n",
    "        DEFAULT_USER_COL: interactions[:, 1],\n",
    "        DEFAULT_ITEM_COL: interactions[:, 2],\n",
    "        DEFAULT_FLAG_COL: interactions[:, 3],\n",
    "        DEFAULT_RATING_COL: interactions[:, 4],\n",
    "        DEFAULT_TIMESTAMP_COL: interactions[:, 5],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_ids</th>\n",
       "      <th>user_ids</th>\n",
       "      <th>item_ids</th>\n",
       "      <th>flag</th>\n",
       "      <th>ratings</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>00261647</td>\n",
       "      <td>4710852001013</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>20001214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>00261647</td>\n",
       "      <td>4711128778882</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>20001214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>00261647</td>\n",
       "      <td>4710467551224</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>20001214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>00261647</td>\n",
       "      <td>4902775015356</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>20001214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>00261647</td>\n",
       "      <td>4710362500051</td>\n",
       "      <td>train</td>\n",
       "      <td>1</td>\n",
       "      <td>20001214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  order_ids  user_ids       item_ids   flag ratings  timestamp\n",
       "0         0  00261647  4710852001013  train       1  20001214 \n",
       "1         0  00261647  4711128778882  train       1  20001214 \n",
       "2         0  00261647  4710467551224  train       1  20001214 \n",
       "3         0  00261647  4902775015356  train       1  20001214 \n",
       "4         0  00261647  4710362500051  train       1  20001214 "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row data staticstics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(464118, 77202, 9238, 7973)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_interact = len(full_data.index)\n",
    "n_orders = full_data[DEFAULT_ORDER_COL].nunique()\n",
    "n_users = full_data[DEFAULT_USER_COL].nunique()\n",
    "n_items = full_data[DEFAULT_ITEM_COL].nunique()\n",
    "(n_interact, n_orders, n_users, n_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "order_ids             9999\n",
       "user_ids          20002000\n",
       "item_ids     9789578032880\n",
       "flag                 train\n",
       "ratings                  1\n",
       "timestamp         20010228\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the integrity of the saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(464118, 77202, 9238, 7973)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_interact = len(full_data.index)\n",
    "n_orders = full_data[DEFAULT_ORDER_COL].nunique()\n",
    "n_users = full_data[DEFAULT_USER_COL].nunique()\n",
    "n_items = full_data[DEFAULT_ITEM_COL].nunique()\n",
    "(n_interact, n_orders, n_users, n_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_neg_sample(eval_df, negative_num, item_sampler):\n",
    "    print(\"sampling negative items...\")\n",
    "    interact_status = eval_df.groupby([\"user_ids\"])[\"item_ids\"].apply(set).reset_index()\n",
    "    total_interact = pd.DataFrame(\n",
    "        {\"user_ids\": [], \"item_ids\": [], \"ratings\": []}, dtype=np.long\n",
    "    )\n",
    "    for index, user_items in interact_status.iterrows():\n",
    "        u = int(user_items[\"user_ids\"])\n",
    "        items = set(user_items[\"item_ids\"])  # item set for user u\n",
    "        n_items = len(items)  # number of positive item for user u\n",
    "        sample_neg_items = set(\n",
    "            item_sampler.sample(negative_num + n_items, 1, True)\n",
    "        )  # first sample negative_num+n_items items\n",
    "        sample_neg_items = list(sample_neg_items - items)[:negative_num]\n",
    "        # filter the positive items and truncate the first negative_num\n",
    "        #     print(len(sample_neg_items))\n",
    "        tp_items = np.append(list(items), sample_neg_items)\n",
    "        #     print(len(tp_items))\n",
    "\n",
    "        tp_users = np.array([1] * (negative_num + n_items), dtype=np.long) * u\n",
    "        tp_ones = np.ones(n_items, dtype=np.long)\n",
    "        tp_zeros = np.zeros(negative_num, dtype=np.long)\n",
    "        ratings = np.append(tp_ones, tp_zeros)\n",
    "        #     print(len(tp_users)),print(len(tp_items)),print(len(ratings))\n",
    "        tp = pd.DataFrame(\n",
    "            {\"user_ids\": tp_users, \"item_ids\": tp_items, \"ratings\": ratings}\n",
    "        )\n",
    "        total_interact = total_interact.append(tp)\n",
    "\n",
    "    total_interact = sklearn.utils.shuffle(total_interact)\n",
    "    return total_interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  divide data into train, validata and test sets, where validata set is in the train set  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test_valid_by_orders(seq_data, validate_size=0.2, test_size=0.2):\n",
    "    print(\"split_test_valid_by_orders\")\n",
    "    seq_data[\"flag\"] == \"train\"\n",
    "    orders = seq_data[DEFAULT_ORDER_COL].unique()\n",
    "    total_size = len(orders)\n",
    "    validate_size = int(total_size * validate_size)\n",
    "    test_size = int(total_size * test_size)\n",
    "    np.sort(orders)\n",
    "    seq_data.loc[\n",
    "        seq_data[DEFAULT_ORDER_COL].isin(orders[total_size - test_size :]), \"flag\"\n",
    "    ] = \"test\"  # the last 20% of the total orders to be the test set\n",
    "    seq_data.loc[\n",
    "        seq_data[DEFAULT_ORDER_COL].isin(orders[: total_size - test_size]), \"flag\"\n",
    "    ] = \"train\"  # the other 80% of the total orders to be the test set\n",
    "    #     np.random.shuffle(orders[:validate_size])\n",
    "    # the last 20% of the training orders to be the validating set\n",
    "    unique_user_ids_test = seq_data[seq_data[\"flag\"] == \"test\"][\n",
    "        DEFAULT_USER_COL\n",
    "    ].unique()\n",
    "    unique_user_ids_train = seq_data[seq_data[\"flag\"] != \"test\"][\n",
    "        DEFAULT_USER_COL\n",
    "    ].unique()\n",
    "    unique_item_ids_test = seq_data[seq_data[\"flag\"] == \"test\"][\n",
    "        DEFAULT_ITEM_COL\n",
    "    ].unique()\n",
    "    unique_item_ids_train = seq_data[seq_data[\"flag\"] != \"test\"][\n",
    "        DEFAULT_ITEM_COL\n",
    "    ].unique()\n",
    "\n",
    "    seq_data.loc[\n",
    "        seq_data[DEFAULT_ORDER_COL].isin(\n",
    "            orders[total_size - test_size - validate_size : total_size - test_size]\n",
    "        ),\n",
    "        \"flag\",\n",
    "    ] = \"validate\"\n",
    "    # seq_data.drop('time', axis = 1, inplace = True)\n",
    "    print(\"labeling train validate test dataset finished \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter data by count of users, items and orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by group_col and filter filter_col that has less num unique() count\n",
    "def fiter_by_count(tp, group_col, filter_col, num):\n",
    "    ordercount = (\n",
    "        tp.groupby([group_col])[filter_col].nunique().rename(\"count\").reset_index()\n",
    "    )\n",
    "    filter_tp = tp[\n",
    "        tp[group_col].isin(ordercount[ordercount[\"count\"] >= num][group_col])\n",
    "    ]\n",
    "    return filter_tp\n",
    "\n",
    "\n",
    "# filter data by the minimum purcharce number of items and users\n",
    "def filter_triplets(tp, min_u_c=5, min_i_c=5, min_o_c=5):\n",
    "    print(\"filter data by the minimum purcharce number of items and users and orders\")\n",
    "    n_interact = len(tp.index)\n",
    "    n_orders = tp[DEFAULT_ORDER_COL].nunique()\n",
    "    n_users = tp[DEFAULT_USER_COL].nunique()\n",
    "    n_items = tp[DEFAULT_ITEM_COL].nunique()\n",
    "    print(\"before filter\", n_interact, n_orders, n_users, n_items)\n",
    "    # Filter users by mixmum number of orders\n",
    "    if min_o_c > 0:\n",
    "        tp = fiter_by_count(tp, DEFAULT_USER_COL, DEFAULT_ORDER_COL, min_o_c)\n",
    "\n",
    "    # Filter users by mixmum number of items\n",
    "    if min_i_c > 0:\n",
    "        tp = fiter_by_count(tp, DEFAULT_USER_COL, DEFAULT_ITEM_COL, min_i_c)\n",
    "\n",
    "    # Filter items by mixmum number of users\n",
    "    if min_u_c > 0:\n",
    "        tp = fiter_by_count(tp, DEFAULT_ITEM_COL, DEFAULT_USER_COL, min_u_c)\n",
    "\n",
    "    n_interact = len(tp.index)\n",
    "    n_orders = tp[DEFAULT_ORDER_COL].nunique()\n",
    "    n_users = tp[DEFAULT_USER_COL].nunique()\n",
    "    n_items = tp[DEFAULT_ITEM_COL].nunique()\n",
    "    print(\"after filter\", n_interact, n_orders, n_users, n_items)\n",
    "    # Update both usercount and itemcount after filtering\n",
    "    # usercount, itemcount = get_count(tp, 'user_ids'), get_count(tp, 'item_ids')\n",
    "    return tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_train_to_files(data, data_base_dir):\n",
    "\n",
    "    user_ids = data[DEFAULT_USER_COL].to_numpy(dtype=np.long)\n",
    "    item_ids = data[DEFAULT_ITEM_COL].to_numpy(dtype=np.long)\n",
    "    order_ids = data[DEFAULT_ORDER_COL].to_numpy(dtype=np.long)\n",
    "    timestamps = data[DEFAULT_TIMESTAMP_COL].to_numpy(dtype=np.long)\n",
    "    ratings = data[DEFAULT_RATING_COL].to_numpy(dtype=np.float32)\n",
    "    data_file = os.path.join(data_base_dir, \"leave_one_item\")\n",
    "    if not os.path.exists(data_file):\n",
    "        os.makedirs(data_file)\n",
    "    data_file = os.path.join(data_file, \"train.npz\")\n",
    "    np.savez_compressed(\n",
    "        data_file,\n",
    "        user_ids=user_ids,\n",
    "        item_ids=item_ids,\n",
    "        order_ids=order_ids,\n",
    "        timestamp=timestamps,\n",
    "        ratings=ratings,\n",
    "    )\n",
    "    print(\n",
    "        \"Data saving to file:\",\n",
    "        data_base_dir,\n",
    "        \"max_item_num:\",\n",
    "        np.max(item_ids),\n",
    "        \"max_user_num:\",\n",
    "        np.max(user_ids),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_to_files(data, data_base_dir, suffix):\n",
    "    user_ids = data[DEFAULT_USER_COL].to_numpy(dtype=np.long)\n",
    "    item_ids = data[DEFAULT_ITEM_COL].to_numpy(dtype=np.long)\n",
    "    ratings = data[DEFAULT_RATING_COL].to_numpy(dtype=np.float32)\n",
    "    data_file = os.path.join(data_base_dir, \"leave_one_item\")\n",
    "    if not os.path.exists(data_file):\n",
    "        os.makedirs(data_file)\n",
    "    data_file = os.path.join(data_file, suffix)\n",
    "    np.savez_compressed(\n",
    "        data_file, user_ids=user_ids, item_ids=item_ids, ratings=ratings,\n",
    "    )\n",
    "    print(\n",
    "        \"Data saving to file:\",\n",
    "        data_base_dir,\n",
    "        \"max_item_num:\",\n",
    "        np.max(item_ids),\n",
    "        \"max_user_num:\",\n",
    "        np.max(user_ids),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiter_full_data = fiter_by_count(full_data, DEFAULT_ITEM_COL, DEFAULT_USER_COL, 10)\n",
    "fiter_full_data = fiter_by_count(fiter_full_data, DEFAULT_USER_COL, DEFAULT_ITEM_COL, 3)\n",
    "users = fiter_full_data[DEFAULT_USER_COL].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9238/9238 [50:32<00:00,  3.05it/s] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for user in tqdm(users):\n",
    "    top_2_item_index = (\n",
    "        fiter_full_data[fiter_full_data[DEFAULT_USER_COL] == user]\n",
    "        .sort_values(by=[DEFAULT_TIMESTAMP_COL], ascending=False)\n",
    "        .head(2)\n",
    "        .index\n",
    "    )\n",
    "    fiter_full_data.loc[top_2_item_index[0], [DEFAULT_FLAG_COL]] = \"test\"\n",
    "    fiter_full_data.loc[top_2_item_index[1], [DEFAULT_FLAG_COL]] = \"validate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling negatives and saving\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9789578032880 max_user_num: 20002000\n",
      "Filling unigram table\n",
      "sampling negative items...\n",
      "sampling negative items...\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9557615168355 max_user_num: 20002000\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9789578032880 max_user_num: 20002000\n",
      "sampling negative items...\n",
      "sampling negative items...\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9557615168355 max_user_num: 20002000\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9789578032880 max_user_num: 20002000\n",
      "sampling negative items...\n",
      "sampling negative items...\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9557615168355 max_user_num: 20002000\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9789578032880 max_user_num: 20002000\n",
      "sampling negative items...\n",
      "sampling negative items...\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9557615168355 max_user_num: 20002000\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9789578032880 max_user_num: 20002000\n",
      "sampling negative items...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-93185ebe8101>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mitem_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnigramTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDEFAULT_ITEM_COL\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtp_validate_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_neg_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp_validate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_sampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtp_test_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_neg_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_sampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0msave_test_to_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp_validate_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_base_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"valid\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-d7b959d2bbc4>\u001b[0m in \u001b[0;36mfeed_neg_sample\u001b[0;34m(eval_df, negative_num, item_sampler)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;34m\"user_ids\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtp_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"item_ids\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtp_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ratings\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mratings\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         )\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mtotal_interact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_interact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mtotal_interact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_interact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"sampling negatives and saving\")\n",
    "data_base_dir = \"../../datasets/tafeng/\"\n",
    "\n",
    "tp_train = fiter_full_data[fiter_full_data[DEFAULT_FLAG_COL] == \"train\"]\n",
    "tp_validate = fiter_full_data[fiter_full_data[DEFAULT_FLAG_COL] == \"validate\"]\n",
    "tp_test = fiter_full_data[fiter_full_data[DEFAULT_FLAG_COL] == \"test\"]\n",
    "\n",
    "save_train_to_files(tp_train, data_base_dir)\n",
    "item_sampler = UnigramTable(tp_train[DEFAULT_ITEM_COL].value_counts().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling negative items...\n",
      "sampling negative items...\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9557615168355 max_user_num: 20002000\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9789578032880 max_user_num: 20002000\n",
      "sampling negative items...\n",
      "sampling negative items...\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9557615168355 max_user_num: 20002000\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9789578032880 max_user_num: 20002000\n",
      "sampling negative items...\n",
      "sampling negative items...\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9557615168355 max_user_num: 20002000\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9789578032880 max_user_num: 20002000\n",
      "sampling negative items...\n",
      "sampling negative items...\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9557615168355 max_user_num: 20002000\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9789578032880 max_user_num: 20002000\n",
      "sampling negative items...\n",
      "sampling negative items...\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9557615168355 max_user_num: 20002000\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9789578032880 max_user_num: 20002000\n",
      "sampling negative items...\n",
      "sampling negative items...\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9557615168355 max_user_num: 20002000\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9789578032880 max_user_num: 20002000\n",
      "sampling negative items...\n",
      "sampling negative items...\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9557615168355 max_user_num: 20002000\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9789578032880 max_user_num: 20002000\n",
      "sampling negative items...\n",
      "sampling negative items...\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9557615168355 max_user_num: 20002000\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9789578032880 max_user_num: 20002000\n",
      "sampling negative items...\n",
      "sampling negative items...\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9557615168355 max_user_num: 20002000\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9789578032880 max_user_num: 20002000\n",
      "sampling negative items...\n",
      "sampling negative items...\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9557615168355 max_user_num: 20002000\n",
      "Data saving to file: ../../datasets/tafeng/ max_item_num: 9789578032880 max_user_num: 20002000\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    tp_validate_new = feed_neg_sample(tp_validate, 100, item_sampler)\n",
    "    tp_test_new = feed_neg_sample(tp_test, 100, item_sampler)\n",
    "    save_test_to_files(tp_validate_new, data_base_dir, suffix=\"valid\" + \"_\" + str(i))\n",
    "    save_test_to_files(tp_test_new, data_base_dir, suffix=\"test\" + \"_\" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(444207, 9238, 9238)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tp_train.index),len(tp_validate.index),len(tp_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "order_ids    74314\n",
       "user_ids      9238\n",
       "item_ids      7857\n",
       "flag             1\n",
       "ratings          1\n",
       "timestamp      216\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp_train.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
